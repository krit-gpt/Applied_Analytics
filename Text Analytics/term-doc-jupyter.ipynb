{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Jupyter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stat 656 - Week 9 Homework Solution\n",
    "This is a solution to the week 9. The main purpose of this assignment is to investigate tokenization, parts of speech tagging, removing stop words and stemming.\n",
    "\n",
    "Data\n",
    "To illustrate the basic concepts behind the analysis of text, a short collection of 8 documents will be used available from http://www.gutenberg.org .\n",
    "\n",
    "Import Packages\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "# NTLK is the Natural Language Tool Kit\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.probability import FreqDist\n",
    "# The following two packages are used to sort the term/doc matrix\n",
    "from collections import Counter\n",
    "import operator\n",
    "Download NLTK Supporting Files\n",
    "The NLTK package uses several supporting files. These need to be downloaded, but only once. Download them initially using the following statements. After these execute successfully, comment them out of your code.\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "[nltk_data] Downloading package punkt to /Users/Home/nltk_data...\n",
    "[nltk_data]   Package punkt is already up-to-date!\n",
    "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
    "[nltk_data]     /Users/Home/nltk_data...\n",
    "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
    "[nltk_data]       date!\n",
    "[nltk_data] Downloading package stopwords to /Users/Home/nltk_data...\n",
    "[nltk_data]   Package stopwords is already up-to-date!\n",
    "[nltk_data] Downloading package wordnet to /Users/Home/nltk_data...\n",
    "[nltk_data]   Package wordnet is already up-to-date!\n",
    "True\n",
    "Create Program Control Attributes\n",
    "The files list is a list of the documents that will be processed. The remaing attributes are used to turn on and off tagging, stop words and stemming.\n",
    "\n",
    "file_path = '/Users/Home/Desktop/python/Text/TextFiles/'\n",
    "files = ['T1.txt', 'T2.txt', 'T3.txt', 'T4.txt', 'T5.txt', 'T6.txt', \\\n",
    "         'T7.txt', 'T8.txt']\n",
    "pos_tags = True\n",
    "stemming = True\n",
    "remove_stop = True\n",
    "Tokenization, POS Tagging, Stop Removal & Stemming\n",
    "With 8 documents, it is best to do everything inside a large loop, reading each document and then processing that document before creating the final term/document matrix.\n",
    "\n",
    "term_doc = []\n",
    "for file in files:\n",
    "    with open (file_path+file, \"r\") as text_file:\n",
    "        adoc = text_file.read()\n",
    "    # Convert to all lower case - required\n",
    "    adoc = (\"%s\" %adoc).lower()\n",
    "    # Replace special characters with spaces\n",
    "    adoc = adoc.replace('-', ' ')\n",
    "    adoc = adoc.replace('_', ' ')\n",
    "    adoc = adoc.replace(',', ' ')\n",
    "    # Replace not contraction with not\n",
    "    adoc = adoc.replace(\"'nt\", \" not\")\n",
    "    adoc = adoc.replace(\"n't\", \" not\")\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(adoc)\n",
    "    tokens = [word.replace(',', '') for word in tokens]\n",
    "    tokens = [word for word in tokens if ('*' not in word) and \\\n",
    "              word != \"''\" and word !=\"``\"]\n",
    "    for word in tokens:\n",
    "        word = re.sub(r'[^\\w\\d\\s]+','',word)\n",
    "    print(\"\\nDocument \"+file+\" contains a total of\", len(tokens), \" terms.\")\n",
    "    \n",
    "    if pos_tags:\n",
    "        # POS Tagging\n",
    "        tokens = nltk.pos_tag(tokens)\n",
    "    if remove_stop:\n",
    "        # Remove stop words\n",
    "        stop = stopwords.words('english') + list(string.punctuation)\n",
    "        stop.append(\"said\")\n",
    "        tokens = [word for word in tokens if word[0] not in stop]\n",
    "        # Remove single character words and simple punctuation\n",
    "        tokens = [word for word in tokens if len(word) > 1]\n",
    "        # Remove numbers and possive \"'s\"\n",
    "        tokens = [word for word in tokens \\\n",
    "                       if (not word[0].replace('.','',1).isnumeric()) and \\\n",
    "                       word[0]!=\"'s\" ]\n",
    "    if stemming:\n",
    "        # Lemmatization - Stemming with POS\n",
    "        # WordNet Lematization Stems using POS\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        wn_tags = {'N':wn.NOUN, 'J':wn.ADJ, 'V':wn.VERB, 'R':wn.ADV}\n",
    "        wnl = WordNetLemmatizer()\n",
    "        stemmed_tokens = []\n",
    "        for token in tokens:\n",
    "            term = token[0]\n",
    "            pos  = token[1]\n",
    "            pos  = pos[0]\n",
    "            try:\n",
    "                pos   = wn_tags[pos]\n",
    "                stemmed_tokens.append(wnl.lemmatize(term, pos=pos))\n",
    "            except:\n",
    "                stemmed_tokens.append(stemmer.stem(term))\n",
    "    if stemming:\n",
    "        print(\"Document \"+file+\" contains\", len(stemmed_tokens), \\\n",
    "                  \"terms after stemming.\") \n",
    "        tokens = stemmed_tokens\n",
    "    # Word distribution\n",
    "    #fdist = FreqDist(word for word in stemmed_tokens)\n",
    "    fdist = FreqDist(tokens)\n",
    "    # Use with Wordnet\n",
    "    td= {}\n",
    "    for word, freq in fdist.most_common(2000):\n",
    "        td[word] = freq\n",
    "    term_doc.append(td)\n",
    "Document T1.txt contains a total of 86512  terms.\n",
    "Document T1.txt contains 40072 terms after stemming.\n",
    "\n",
    "Document T2.txt contains a total of 108475  terms.\n",
    "Document T2.txt contains 48290 terms after stemming.\n",
    "\n",
    "Document T3.txt contains a total of 104764  terms.\n",
    "Document T3.txt contains 50163 terms after stemming.\n",
    "\n",
    "Document T4.txt contains a total of 83138  terms.\n",
    "Document T4.txt contains 35273 terms after stemming.\n",
    "\n",
    "Document T5.txt contains a total of 76236  terms.\n",
    "Document T5.txt contains 35031 terms after stemming.\n",
    "\n",
    "Document T6.txt contains a total of 35074  terms.\n",
    "Document T6.txt contains 15740 terms after stemming.\n",
    "\n",
    "Document T7.txt contains a total of 80268  terms.\n",
    "Document T7.txt contains 35592 terms after stemming.\n",
    "\n",
    "Document T8.txt contains a total of 64518  terms.\n",
    "Document T8.txt contains 29804 terms after stemming.\n",
    "Create the Term/Document Matrix\n",
    "The following code creates the term/document matrix by combining the counts for each document. A list of dictionaries term_doc was created for the 8 documents. Each dictionary contains the terms for that document and the count of the number of times it appears in that document.\n",
    "\n",
    "td_mat = {}\n",
    "for td in term_doc:\n",
    "    td_mat = Counter(td_mat)+Counter(td)\n",
    "td_matrix = {}\n",
    "for k, v in td_mat.items():\n",
    "    td_matrix[k] = [v]\n",
    "for td in term_doc:\n",
    "    for k, v in td_matrix.items():\n",
    "        if k in td:\n",
    "            td_matrix[k].append(td[k])\n",
    "        else:\n",
    "            td_matrix[k].append(0)\n",
    "Sort Term/Document Matrix by Total Word Count\n",
    "The term/doc matrix td_matrix must be sorted placing the terms with the largest total word count at the top of the matrix since we want to display the top 20 terms. The 20 terms with the largest word counts.\n",
    "\n",
    "Sorting is done using operator.itemgetter() which sorts a dictionary and returns the result as a list. td_matrix_sorted is a list, not a dictionary.\n",
    "\n",
    "td_matrix_sorted = sorted(td_matrix.items(), key=operator.itemgetter(1),\\\n",
    "                          reverse=True)\n",
    "Display the Top 20 Terms\n",
    "print(\"Scenario: POS=\", pos_tags, \"Remove Stop Words=\", remove_stop, \\\n",
    "      \" Stemming=\", stemming)\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"     TERM      TOTAL   T1   T2   T3   T4   T5   T6   T7   T8\")\n",
    "for i in range(20):\n",
    "    s = '{:<15s}'.format(td_matrix_sorted[i][0])\n",
    "    v = td_matrix_sorted[i][1]\n",
    "    for j in range(9):\n",
    "        s = s + '{:>5d}'.format(v[j])\n",
    "    print('{:<60s}'.format(s))\n",
    "print(\"____________________________________________________________\")\n",
    "Scenario: POS= True Remove Stop Words= True  Stemming= True\n",
    "------------------------------------------------------------\n",
    "     TERM      TOTAL   T1   T2   T3   T4   T5   T6   T7   T8\n",
    "one             2127  291  437  348  211  312  121  202  205\n",
    "water           2040   47  922  825    7   94    7   55   83\n",
    "make            1928  204  694  262  185  237   63  169  114\n",
    "would           1855  270  407  195  309  222   60  289  103\n",
    "go              1620  212  292   18  239  154  103  374  228\n",
    "come            1511  211  153   62  126  276  155  282  246\n",
    "could           1363  221  121   49  364  195   93  203  117\n",
    "time            1333  137  128  175  167  164  213  216  133\n",
    "see             1188  179  232  129  156  110   72  172  138\n",
    "light           1175   87  461  322   21   92   61   60   71\n",
    "get             1147  171  291   24   76  121   53  316   95\n",
    "air             1126   69  518  412   20   19   23   30   35\n",
    "know            1043  165  102  112  223  119   46  203   73\n",
    "day              939   87   52   82  117  337   49  107  108\n",
    "take             926  129  174   82  110  135   52  178   66\n",
    "upon             891  168   11  163   88   28  113  148  172\n",
    "way              844   78  211  122   73  100   42  116  102\n",
    "thing            832  120  241   10   76   57  100  113  115\n",
    "like             828  173  119   56  100   80   74  130   96\n",
    "man              827  248   38  104   90   90   70   62  125\n",
    "____________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This repo contains an introduction to [Jupyter](https://jupyter.org) and [IPython](https://ipython.org).\n",
    "\n",
    "Outline of some basics:\n",
    "\n",
    "* [Notebook Basics](../examples/Notebook/Notebook Basics.ipynb)\n",
    "* [IPython - beyond plain python](../examples/IPython Kernel/Beyond Plain Python.ipynb)\n",
    "* [Markdown Cells](../examples/Notebook/Working With Markdown Cells.ipynb)\n",
    "* [Rich Display System](../examples/IPython Kernel/Rich Output.ipynb)\n",
    "* [Custom Display logic](../examples/IPython%20Kernel/Custom%20Display%20Logic.ipynb)\n",
    "* [Running a Secure Public Notebook Server](../examples/Notebook/Running%20the%20Notebook%20Server.ipynb#Securing-the-notebook-server)\n",
    "* [How Jupyter works](../examples/Notebook/Multiple%20Languages%2C%20Frontends.ipynb) to run code in different languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also get this tutorial and run it on your laptop:\n",
    "\n",
    "    git clone https://github.com/ipython/ipython-in-depth\n",
    "\n",
    "Install IPython and Jupyter:\n",
    "\n",
    "with [conda](https://www.anaconda.com/download):\n",
    "\n",
    "    conda install ipython jupyter\n",
    "\n",
    "with pip:\n",
    "\n",
    "    # first, always upgrade pip!\n",
    "    pip install --upgrade pip\n",
    "    pip install --upgrade ipython jupyter\n",
    "\n",
    "Start the notebook in the tutorial directory:\n",
    "\n",
    "    cd ipython-in-depth\n",
    "    jupyter notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
